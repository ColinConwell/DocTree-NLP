{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# DocTree-NLP Demo\n\nThis notebook demonstrates the core functionalities of the DocTree-NLP library, including:\n- Authentication with Notion API\n- Listing and accessing documents\n- Processing text with NLP capabilities\n- Building document hierarchies\n- Using lazy document loading\n- Working with document windows\n- Automatic tagging and keyword extraction\n\nFirst, let's import the required modules:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom datetime import datetime\n\n# Import core components\nfrom doctree_nlp import (\n    NotionClient,\n    TextProcessor,\n    Tagger,\n    DocTree,\n    DEFAULT_CACHE_DIR\n)\n\n# Import structure components\nfrom doctree_nlp.structure import Document, Block\n\n# Import performance optimization components\nfrom doctree_nlp.lazy_document import LazyDocument, LazyDocumentCollection\nfrom doctree_nlp.windowing import DocumentWindower, DocumentWindow\n\n# Import notebook display helpers\nfrom doctree_nlp.notebook import display_document, display_document_tree, display_document_table"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Authentication\n\nFirst, we'll initialize the Notion client with our API token:\n\n*Note: The library now supports auto-discovery of your token from environment variables.*",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Use auto token discovery (recommended approach)\nclient = NotionClient(\n    token=\"auto\",                       # Auto-discover token from environment variables\n    cache_enabled=True,                 # Enable caching for faster repeat queries\n    cache_dir=DEFAULT_CACHE_DIR,        # Use default cache directory\n    max_cache_age_days=1,               # Cache valid for 1 day\n    rate_limit=3                        # Limit to 3 requests per second (Notion API limit)\n)\n\n# Alternative approach with explicit token:\n# notion_token = os.environ.get('NOTION_API_TOKEN')\n# client = NotionClient(token=notion_token, ...)\n\n# Verify authentication\nauth_status = client.authenticate()\nprint(f\"Authentication status: {'Successful' if auth_status else 'Failed'}\")\n\n# Display cache information\ntry:\n    cache_info = client.get_cache_info()\n    print(f\"\\nCache configuration:\")\n    print(f\"- Cache enabled: {cache_info['enabled']}\")\n    print(f\"- Cache directory: {cache_info['cache_dir']}\")\n    print(f\"- Max age: {cache_info['max_age_days']} days\")\nexcept Exception as e:\n    print(f\"Could not get cache info: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Listing Documents\n\nLet's retrieve and display the list of available documents:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# List documents (uses cache if available)\nprint(\"Fetching documents (will use cache after first run)...\")\ndocuments = client.list_documents()\nprint(f\"Found {len(documents)} documents:\")\nfor doc in documents[:5]:  # Show first 5 documents for brevity\n    print(f\"- {doc.title} (ID: {doc.id})\")\n    print(f\"  Last edited: {doc.last_edited_time}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Lazy Document Loading\n\nThe library now supports lazy loading of documents, which is more memory efficient:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create a lazy document collection\nlazy_collection = LazyDocumentCollection(\n    client=client,\n    preload_metadata=True,  # Preload document metadata but not content\n    load_strategy=\"on_demand\"  # Load content only when accessed\n)\n\n# Print the number of documents in the collection\nprint(f\"Lazy collection contains {len(lazy_collection.documents)} documents\")\n\n# Get a lazy document - content won't be loaded yet\nif documents:  # Use first document from previous list\n    lazy_doc = lazy_collection.get_document(documents[0].id)\n    print(f\"Lazy document: {lazy_doc.title}\")\n    print(f\"Content loaded: {lazy_doc._blocks_loaded}\")  # Should be False initially\n    \n    # Access a property that requires content - this will trigger loading\n    print(f\"\\nAccessing document content...\")\n    preview = lazy_doc.preview_text(n_chars=150)\n    print(f\"Content loaded now: {lazy_doc._blocks_loaded}\")  # Should be True now\n    print(f\"Preview: {preview}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Processing Document Content\n\nNow, let's fetch and process the content of a document:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "if documents:\n    # Get a regular document with all content\n    doc = documents[0]\n    print(f\"Processing document: {doc.title}\")\n    \n    # Get complete document with content\n    document = client.get_document(doc.id)\n    print(f\"Retrieved {len(document.blocks)} blocks from document\")\n    print(f\"Document last edited: {document.last_edited_time}\")\n    print(f\"Last fetched from API: {document.last_fetched}\")\n    \n    # Process text\n    processor = TextProcessor()\n    processed_blocks = processor.process_blocks(document.blocks)\n    \n    # Display results\n    print(\"\\nProcessed content:\")\n    for block in processed_blocks[:2]:  # Show first 2 blocks for brevity\n        print(f\"\\nBlock type: {block['type']}\")\n        print(f\"Entities found: {[e['text'] for e in block['entities']]}\")\n        print(f\"Keywords: {block['keywords']}\")\n        print(f\"Sentences: {len(block['sentences'])}\")\n    \n    # Try forcing a refresh (bypassing cache)\n    print(\"\\nForcing refresh (bypassing cache)...\")\n    refreshed_document = client.get_document(doc.id, use_cache=False)\n    print(f\"Retrieved {len(refreshed_document.blocks)} blocks directly from API\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Document Windowing\n\nFor large documents, you can use windowing to view portions of the document:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "if documents and len(document.blocks) > 0:\n    # Create a document windower\n    windower = DocumentWindower(default_window_size=5)  # 5 blocks per window\n    \n    # Create the first window\n    window = windower.create_window(document)\n    print(f\"Window {window.start_index}-{window.end_index} of {window.total_blocks} blocks\")\n    \n    # Display window blocks\n    for block in window.blocks:\n        print(f\"- [{block.type}] {block.content[:50]}... ({block.id})\")\n    \n    # Get the next window if available\n    if window.has_next:\n        next_window = windower.get_next_window(window, document)\n        print(f\"\\nNext window {next_window.start_index}-{next_window.end_index}:\")\n        for block in next_window.blocks:\n            print(f\"- [{block.type}] {block.content[:50]}...\")\n    \n    # Search for text and create a window around it\n    if len(document.blocks) > 10:\n        search_text = document.blocks[10].content[:20]  # Use part of a block farther down\n        print(f\"\\nSearching for: '{search_text}'\")\n        search_window = windower.find_text_window(\n            document, \n            search_text, \n            window_size=3,\n            context_blocks=1\n        )\n        \n        if search_window:\n            print(f\"Found text in window {search_window.start_index}-{search_window.end_index}\")\n            for block in search_window.blocks:\n                print(f\"- [{block.type}] {block.content[:50]}...\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Building Document Hierarchy\n\nLet's analyze the document's structure using the improved DocTree class:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if documents and hasattr(document, 'blocks') and len(document.blocks) > 0:\n    # Build tree automatically\n    document.build_tree()\n    \n    # Convert to dictionary for visualization\n    structure = document.to_dict()\n    print(\"Document structure:\")\n    print(f\"- Title: {structure['title']}\")\n    print(f\"- Created: {structure['created_time']}\")\n    print(f\"- Last edited: {structure['last_edited_time']}\")\n    \n    # Search for specific nodes in the tree by type\n    if document.tree:\n        headings = document.tree.find_nodes_by_type('heading_1')\n        print(f\"\\nFound {len(headings)} level 1 headings:\")\n        for heading in headings:\n            print(f\"- {heading.block.content}\")\n            \n        # Search for content using regex patterns\n        if len(document.blocks) > 0:\n            # Use the first word of the first block as search term\n            search_term = document.blocks[0].content.split()[0] if document.blocks[0].content else \"the\"\n            matching_nodes = document.tree.find_nodes_by_content(f\".*{search_term}.*\")\n            print(f\"\\nFound {len(matching_nodes)} blocks containing '{search_term}'\")\n    \n    # Check cache status after operations\n    try:\n        cache_info = client.get_cache_info()\n        print(f\"\\nCache status after operations:\")\n        print(f\"- Files in cache: {cache_info['num_files']}\")\n        print(f\"- Cache size: {cache_info['total_size_mb']:.2f} MB\")\n    except Exception as e:\n        print(f\"Could not get cache info: {e}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Jupyter Notebook Integration\n\nThe library includes built-in display functions for Jupyter notebooks:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if documents and document and hasattr(document, 'blocks') and len(document.blocks) > 0:\n    # Display document summary with preview\n    print(\"Document summary:\\n\")\n    display_document(document)\n    \n    # Display blocks as table\n    print(\"\\nDocument blocks table:\\n\")\n    display_document_table(document)\n    \n    # Display document structure as interactive tree\n    if document.tree:\n        print(\"\\nDocument structure tree:\\n\")\n        display_document_tree(document)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Automatic Tagging\n\nFinally, let's generate tags for the document content:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if documents and hasattr(document, 'blocks') and len(document.blocks) > 0:\n    # Initialize tagger\n    tagger = Tagger()\n    \n    # Add some custom tags for matching\n    tagger.add_custom_tags([\"important\", \"review\", \"followup\"])\n    \n    print(\"Generated tags:\")\n    for block in document.blocks[:3]:  # Process first 3 blocks for demonstration\n        tags = tagger.generate_tags(block)\n        print(f\"\\nBlock content: {block.content[:50]}...\")\n        print(f\"Tags: {[tag.name for tag in tags]}\")\n        \n        # Analyze sentiment\n        sentiment = tagger.analyze_sentiment(block.content)\n        print(f\"Sentiment: {sentiment}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Using Alternative Document Sources\n\nThe library also supports loading documents from Obsidian vaults and local directories:\n\n```python\n# Obsidian Client\nfrom doctree_nlp import ObsidianClient\n\nobsidian_client = ObsidianClient(\n    vault_path='/path/to/obsidian/vault',\n    cache_enabled=True\n)\n\nobsidian_docs = obsidian_client.list_documents()\n\n# Local Source Client\nfrom doctree_nlp import LocalSource\n\nlocal_client = LocalSource(\n    directory_path='/path/to/markdown/files',\n    file_pattern='**/*.md',\n    cache_enabled=True\n)\n\nlocal_docs = local_client.list_documents()\n```",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}